{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirishaallarapu/AdvancedPySpark-/blob/main/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgBX1i9BBfXz",
        "outputId": "80f4e67a-f403-4a44-db30-00ced36fb26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------------+\n",
            "|Department|Avg_Salary|Total_Salary|\n",
            "+----------+----------+------------+\n",
            "|   Finance|   65000.0|       65000|\n",
            "| Marketing|   55000.0|       55000|\n",
            "|        IT|   75000.0|      150000|\n",
            "|        HR|   53000.0|       53000|\n",
            "|Operations|   60000.0|       60000|\n",
            "+----------+----------+------------+\n",
            "\n",
            "+------+------+----------+------+\n",
            "|Emp_ID|  Name|Department|Salary|\n",
            "+------+------+----------+------+\n",
            "|   304|Anjali|        HR| 53000|\n",
            "|   303| Manoj| Marketing| 55000|\n",
            "|   306| Pooja|Operations| 60000|\n",
            "|   301|Rajesh|   Finance| 65000|\n",
            "|   302|Sunita|        IT| 72000|\n",
            "|   305| Tarun|        IT| 78000|\n",
            "+------+------+----------+------+\n",
            "\n",
            "+------+------+----------+------+\n",
            "|Emp_ID|  Name|Department|Salary|\n",
            "+------+------+----------+------+\n",
            "|   305| Tarun|        IT| 78000|\n",
            "|   302|Sunita|        IT| 72000|\n",
            "|   301|Rajesh|   Finance| 65000|\n",
            "|   306| Pooja|Operations| 60000|\n",
            "|   303| Manoj| Marketing| 55000|\n",
            "|   304|Anjali|        HR| 53000|\n",
            "+------+------+----------+------+\n",
            "\n",
            "+-----------+-------------+-------+--------+---------------+------+\n",
            "|customer_id|customer_name|   city|order_id|        product|amount|\n",
            "+-----------+-------------+-------+--------+---------------+------+\n",
            "|        201|        Vikas|   Pune|     601|     Smartphone| 45000|\n",
            "|        202|       Kavita|Chennai|     603|   Refrigerator| 35000|\n",
            "|        203|      Sandeep|Kolkata|     602|Washing Machine| 30000|\n",
            "+-----------+-------------+-------+--------+---------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EmployeeOperations\").getOrCreate()\n",
        "\n",
        "employee_data = [\n",
        "    (301, \"Rajesh\", \"Finance\", 65000),\n",
        "    (302, \"Sunita\", \"IT\", 72000),\n",
        "    (303, \"Manoj\", \"Marketing\", 55000),\n",
        "    (304, \"Anjali\", \"HR\", 53000),\n",
        "    (305, \"Tarun\", \"IT\", 78000),\n",
        "    (306, \"Pooja\", \"Operations\", 60000)\n",
        "]\n",
        "employee_columns = [\"Emp_ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(employee_data, employee_columns)\n",
        "\n",
        "df_emp.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Avg_Salary\"), sum(\"Salary\").alias(\"Total_Salary\")).show()\n",
        "\n",
        "df_emp.orderBy(col(\"Salary\").asc()).show()\n",
        "df_emp.orderBy(col(\"Salary\").desc()).show()\n",
        "\n",
        "customer_data = [\n",
        "    (201, \"Vikas\", \"Pune\"),\n",
        "    (202, \"Kavita\", \"Chennai\"),\n",
        "    (203, \"Sandeep\", \"Kolkata\"),\n",
        "    (204, \"Meghna\", \"Jaipur\")\n",
        "]\n",
        "customer_columns = [\"customer_id\", \"customer_name\", \"city\"]\n",
        "df_customers = spark.createDataFrame(customer_data, customer_columns)\n",
        "\n",
        "order_data = [\n",
        "    (601, 201, \"Smartphone\", 45000),\n",
        "    (602, 203, \"Washing Machine\", 30000),\n",
        "    (603, 202, \"Refrigerator\", 35000),\n",
        "    (604, 205, \"Microwave\", 10000)\n",
        "]\n",
        "order_columns = [\"order_id\", \"customer_id\", \"product\", \"amount\"]\n",
        "df_orders = spark.createDataFrame(order_data, order_columns)\n",
        "\n",
        "df_customers.join(df_orders, on=\"customer_id\", how=\"inner\").show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, max, min, round\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySparkExamples\").getOrCreate()\n",
        "\n",
        "student_data = [\n",
        "    (101, \"Aarav\", \"Math\", 85),\n",
        "    (102, \"Ishika\", \"Science\", 78),\n",
        "    (103, \"Rohan\", \"Math\", 92),\n",
        "    (104, \"Meera\", \"English\", 88),\n",
        "    (105, \"Neel\", \"Science\", 74),\n",
        "    (106, \"Tanvi\", \"English\", 91),\n",
        "    (107, \"Vihan\", \"Math\", 89),\n",
        "    (108, \"Sneha\", \"Science\", 81)\n",
        "]\n",
        "student_columns = [\"Student_ID\", \"Name\", \"Subject\", \"Marks\"]\n",
        "df_students = spark.createDataFrame(student_data, student_columns)\n",
        "\n",
        "df_students.groupBy(\"Subject\").agg(round(avg(\"Marks\"), 2).alias(\"Avg_Marks\"), max(\"Marks\").alias(\"Max_Marks\"), min(\"Marks\").alias(\"Min_Marks\")).show()\n",
        "\n",
        "df_students.orderBy(col(\"Marks\").asc()).show()\n",
        "df_students.orderBy(col(\"Marks\").desc()).show()\n",
        "\n",
        "book_data = [\n",
        "    (201, \"Ramayana\", \"Valmiki\"),\n",
        "    (202, \"Mahabharata\", \"Vyasa\"),\n",
        "    (203, \"Wings of Fire\", \"APJ Abdul Kalam\"),\n",
        "    (204, \"The Guide\", \"R.K. Narayan\"),\n",
        "    (205, \"Gitanjali\", \"Rabindranath Tagore\")\n",
        "]\n",
        "book_columns = [\"Book_ID\", \"Title\", \"Author\"]\n",
        "df_books = spark.createDataFrame(book_data, book_columns)\n",
        "\n",
        "borrow_data = [\n",
        "    (301, 201, \"Aarav\"),\n",
        "    (302, 203, \"Ishika\"),\n",
        "    (303, 202, \"Rohan\"),\n",
        "    (304, 204, \"Meera\"),\n",
        "    (305, 205, \"Tanvi\"),\n",
        "    (306, 206, \"Neel\")\n",
        "]\n",
        "borrow_columns = [\"Transaction_ID\", \"Book_ID\", \"Student_Name\"]\n",
        "df_borrow = spark.createDataFrame(borrow_data, borrow_columns)\n",
        "\n",
        "df_books.join(df_borrow, on=\"Book_ID\", how=\"inner\").show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmEY77ICVYH",
        "outputId": "067fe25d-7add-44ea-b6d0-b5c0555521df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+---------+\n",
            "|Subject|Avg_Marks|Max_Marks|Min_Marks|\n",
            "+-------+---------+---------+---------+\n",
            "|Science|    77.67|       81|       74|\n",
            "|   Math|    88.67|       92|       85|\n",
            "|English|     89.5|       91|       88|\n",
            "+-------+---------+---------+---------+\n",
            "\n",
            "+----------+------+-------+-----+\n",
            "|Student_ID|  Name|Subject|Marks|\n",
            "+----------+------+-------+-----+\n",
            "|       105|  Neel|Science|   74|\n",
            "|       102|Ishika|Science|   78|\n",
            "|       108| Sneha|Science|   81|\n",
            "|       101| Aarav|   Math|   85|\n",
            "|       104| Meera|English|   88|\n",
            "|       107| Vihan|   Math|   89|\n",
            "|       106| Tanvi|English|   91|\n",
            "|       103| Rohan|   Math|   92|\n",
            "+----------+------+-------+-----+\n",
            "\n",
            "+----------+------+-------+-----+\n",
            "|Student_ID|  Name|Subject|Marks|\n",
            "+----------+------+-------+-----+\n",
            "|       103| Rohan|   Math|   92|\n",
            "|       106| Tanvi|English|   91|\n",
            "|       107| Vihan|   Math|   89|\n",
            "|       104| Meera|English|   88|\n",
            "|       101| Aarav|   Math|   85|\n",
            "|       108| Sneha|Science|   81|\n",
            "|       102|Ishika|Science|   78|\n",
            "|       105|  Neel|Science|   74|\n",
            "+----------+------+-------+-----+\n",
            "\n",
            "+-------+-------------+-------------------+--------------+------------+\n",
            "|Book_ID|        Title|             Author|Transaction_ID|Student_Name|\n",
            "+-------+-------------+-------------------+--------------+------------+\n",
            "|    201|     Ramayana|            Valmiki|           301|       Aarav|\n",
            "|    202|  Mahabharata|              Vyasa|           303|       Rohan|\n",
            "|    203|Wings of Fire|    APJ Abdul Kalam|           302|      Ishika|\n",
            "|    204|    The Guide|       R.K. Narayan|           304|       Meera|\n",
            "|    205|    Gitanjali|Rabindranath Tagore|           305|       Tanvi|\n",
            "+-------+-------------+-------------------+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, avg, max, min\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySparkExamples\").getOrCreate()\n",
        "\n",
        "vehicle_data = [\n",
        "    (1, \"Hyundai\", \"Sedan\", 15000),\n",
        "    (2, \"Toyota\", \"SUV\", 22000),\n",
        "    (3, \"Honda\", \"Sedan\", 18000),\n",
        "    (4, \"Ford\", \"Truck\", 25000),\n",
        "    (5, \"Tata\", \"SUV\", 20000),\n",
        "    (6, \"Mahindra\", \"Truck\", 27000),\n",
        "    (7, \"Maruti\", \"Hatchback\", 12000),\n",
        "    (8, \"Kia\", \"SUV\", 23000)\n",
        "]\n",
        "vehicle_columns = [\"Vehicle_ID\", \"Brand\", \"Category\", \"Price\"]\n",
        "df_vehicles = spark.createDataFrame(vehicle_data, vehicle_columns)\n",
        "\n",
        "df_vehicles.groupBy(\"Category\").agg(count(\"Vehicle_ID\").alias(\"Total_Vehicles\"), avg(\"Price\").alias(\"Avg_Price\"), max(\"Price\").alias(\"Max_Price\"), min(\"Price\").alias(\"Min_Price\")).show()\n",
        "\n",
        "df_vehicles.orderBy(col(\"Price\").asc()).show()\n",
        "df_vehicles.orderBy(col(\"Price\").desc()).show()\n",
        "\n",
        "dealer_data = [\n",
        "    (101, \"Auto World\", \"Mumbai\"),\n",
        "    (102, \"Speed Motors\", \"Delhi\"),\n",
        "    (103, \"Highway Cars\", \"Bangalore\"),\n",
        "    (104, \"Urban Wheels\", \"Pune\")\n",
        "]\n",
        "dealer_columns = [\"Dealer_ID\", \"Dealer_Name\", \"City\"]\n",
        "df_dealers = spark.createDataFrame(dealer_data, dealer_columns)\n",
        "\n",
        "sales_data = [\n",
        "    (201, 101, \"Hyundai\", 10),\n",
        "    (202, 103, \"Toyota\", 8),\n",
        "    (203, 102, \"Honda\", 12),\n",
        "    (204, 104, \"Ford\", 5),\n",
        "    (205, 101, \"Tata\", 7),\n",
        "    (206, 103, \"Mahindra\", 6)\n",
        "]\n",
        "sales_columns = [\"Sale_ID\", \"Dealer_ID\", \"Brand\", \"Units_Sold\"]\n",
        "df_sales = spark.createDataFrame(sales_data, sales_columns)\n",
        "\n",
        "df_dealers.join(df_sales, on=\"Dealer_ID\", how=\"inner\").show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyxW4j1UChRb",
        "outputId": "ac32d111-def2-4554-9e6b-2b67012c65e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+------------------+---------+---------+\n",
            "| Category|Total_Vehicles|         Avg_Price|Max_Price|Min_Price|\n",
            "+---------+--------------+------------------+---------+---------+\n",
            "|      SUV|             3|21666.666666666668|    23000|    20000|\n",
            "|    Sedan|             2|           16500.0|    18000|    15000|\n",
            "|    Truck|             2|           26000.0|    27000|    25000|\n",
            "|Hatchback|             1|           12000.0|    12000|    12000|\n",
            "+---------+--------------+------------------+---------+---------+\n",
            "\n",
            "+----------+--------+---------+-----+\n",
            "|Vehicle_ID|   Brand| Category|Price|\n",
            "+----------+--------+---------+-----+\n",
            "|         7|  Maruti|Hatchback|12000|\n",
            "|         1| Hyundai|    Sedan|15000|\n",
            "|         3|   Honda|    Sedan|18000|\n",
            "|         5|    Tata|      SUV|20000|\n",
            "|         2|  Toyota|      SUV|22000|\n",
            "|         8|     Kia|      SUV|23000|\n",
            "|         4|    Ford|    Truck|25000|\n",
            "|         6|Mahindra|    Truck|27000|\n",
            "+----------+--------+---------+-----+\n",
            "\n",
            "+----------+--------+---------+-----+\n",
            "|Vehicle_ID|   Brand| Category|Price|\n",
            "+----------+--------+---------+-----+\n",
            "|         6|Mahindra|    Truck|27000|\n",
            "|         4|    Ford|    Truck|25000|\n",
            "|         8|     Kia|      SUV|23000|\n",
            "|         2|  Toyota|      SUV|22000|\n",
            "|         5|    Tata|      SUV|20000|\n",
            "|         3|   Honda|    Sedan|18000|\n",
            "|         1| Hyundai|    Sedan|15000|\n",
            "|         7|  Maruti|Hatchback|12000|\n",
            "+----------+--------+---------+-----+\n",
            "\n",
            "+---------+------------+---------+-------+--------+----------+\n",
            "|Dealer_ID| Dealer_Name|     City|Sale_ID|   Brand|Units_Sold|\n",
            "+---------+------------+---------+-------+--------+----------+\n",
            "|      101|  Auto World|   Mumbai|    201| Hyundai|        10|\n",
            "|      101|  Auto World|   Mumbai|    205|    Tata|         7|\n",
            "|      102|Speed Motors|    Delhi|    203|   Honda|        12|\n",
            "|      103|Highway Cars|Bangalore|    202|  Toyota|         8|\n",
            "|      103|Highway Cars|Bangalore|    206|Mahindra|         6|\n",
            "|      104|Urban Wheels|     Pune|    204|    Ford|         5|\n",
            "+---------+------------+---------+-------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}